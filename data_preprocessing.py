# -*- coding: utf-8 -*-
"""Data Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wjFG-x1hpLyjQ_OE4HoDlCAiQ-MOo5r7
"""

# !pip install pymongo

"""# Pymongo Config"""

from pymongo import MongoClient
import pandas as pd
from pprint import pprint

# Configuration
MONGO_URI = "mongodb+srv://trandoanvuongtu:HB1j39319UxU2iHY@cluster0.2cudz7h.mongodb.net/?retryWrites=true&w=majority"
DATABASE_NAME = "Film"
COLLECTION_NAME = "Data"

def connect_to_mongodb(mongo_uri):
    """Function to connect to mongodb client.
    Return mongodb client if successfull."""

    # Connect to server
    mongo_client = MongoClient(mongo_uri)

    # Ping to server
    try:
      mongo_client.admin.command('ping')
      print("Pinged your deployment. You successfully connected to MongoDB!")
    except Exception as e:
      raise Exception(f"Connection failed: {e}")

    return mongo_client

def get_database(mongo_client, database_name):
    """Function to get database from mongodb client."
    Return database if successfull."""

    # Get database
    try:
      database = mongo_client[database_name]
      print(f"Database '{database_name}' connected successfully!")
    except Exception as e:
      raise Exception(f"Connection failed: {e}")

    return database

def get_collection(database, collection_name):
    """Function to get collection from database."
    Return collection if successfull."""

    # Get collection
    try:
      collection = database[collection_name]
      print(f"Collection '{collection_name}' connected successfully!")
    except Exception as e:
      raise Exception(f"Connection failed: {e}")

    return collection

def get_all_documents(collection):
    """Function to get all documents from collection.
        Return list of documents.
    """
    all_documents = []
    try:
        documents = collection.find()
        for document in documents:
          all_documents.append(document)
    except Exception as e:
      raise Exception(f"Connection failed: {e}")

    return all_documents

def get_documents_by_index(collection, start_index, end_index):
    """Function to get documents from collection by index.
        Return list of documents.
    """
    documents = []
    try:
        documents = collection.find().skip(start_index).limit(end_index)
    except Exception as e:
      raise Exception(f"Connection failed: {e}")

    return documents

# Connect to MongoDB and get collection
mongo_client = connect_to_mongodb(MONGO_URI)
database = get_database(mongo_client, DATABASE_NAME)
collection = get_collection(database, COLLECTION_NAME)

# Get all documents
all_documents = get_all_documents(collection)

# Print some samples
all_documents[:3]

"""# Preprocessing"""

# !pip -q install contractions unidecode

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import contractions
from typing import List, Optional, Set

# Ensure NLTK data is downloaded
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt_tab', quiet=True)

"""## LSA/SVD preprocessing

"""

class LSASVDPipeline:
    """
    Preprocessing pipeline for LSA/SVD
    """

    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    html_pattern = re.compile(r'<[^>]+>')
    non_alpha_pattern = re.compile(r'[^a-zA-Z\s]')

    def __init__(
        self,
        extra_stopwords: Optional[Set[str]] = None
    ):
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        base_sw = set(stopwords.words('english'))
        self.stop_words = base_sw.union(extra_stopwords or set())

    def clean_text(self, text: str) -> str:
        if not isinstance(text, str):
            return ""
        # lowercase
        text = text.lower()
        # remove HTML
        text = self.html_pattern.sub('', text)
        # remove URLs
        text = self.url_pattern.sub('', text)
        # remove non-alphabetic
        text = self.non_alpha_pattern.sub(' ', text)
        # normalize spaces
        return re.sub(r'\s+', ' ', text).strip()

    def tokenize_filter(self, text: str) -> List[str]:
        tokens = word_tokenize(text)
        return [tok for tok in tokens if tok not in self.stop_words and len(tok) > 2]

    def stem(self, tokens: List[str]) -> List[str]:
        return [self.stemmer.stem(tok) for tok in tokens]

    def lemmatize(self, tokens: List[str]) -> List[str]:
        return [self.lemmatizer.lemmatize(tok) for tok in tokens]

    def preprocess(self, text: str, use_stemming: bool = True) -> str:
        cleaned = self.clean_text(text)
        tokens = self.tokenize_filter(cleaned)
        processed = self.stem(tokens) if use_stemming else self.lemmatize(tokens)
        return ' '.join(processed)

    def batch(self, texts: List[str], use_stemming: bool = True) -> List[str]:
        return [self.preprocess(txt, use_stemming) for txt in texts]

"""## WordEmbedding preprocessing"""

class WordEmbeddingPipeline:
    """
    Preprocessing pipeline for WordEmbedding (Word2Vec/GloVe/FastText)
    --> CORE: Lightweight preprocessing. Maintain context.
    """

    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    html_pattern = re.compile(r'<[^>]+>')
    punct_except_basic = re.compile(r'[^\w\s\.\!?]')

    def __init__(self, minimal_stopwords: Optional[Set[str]] = None):
        self.lemmatizer = WordNetLemmatizer()
        defaults = {"a", "an", "the", "and", "or", "but", "is", "are", "was", "were"}
        self.stop_words = defaults.union(minimal_stopwords or set())

    def expand_contractions(self, text: str) -> str:
        return contractions.fix(text)

    def clean_gentle(self, text: str) -> str:
        if not isinstance(text, str):
            return ""
        text = self.expand_contractions(text)
        text = text.lower()
        text = self.html_pattern.sub('', text)
        text = self.url_pattern.sub(' ', text)
        text = self.punct_except_basic.sub(' ', text)
        return re.sub(r'\s+', ' ', text).strip()

    def tokenize_sentences(self, text: str) -> List[List[str]]:
        sentences = sent_tokenize(text)
        result = []
        for sent in sentences:
            toks = word_tokenize(sent)
            filtered = [tok for tok in toks if tok not in self.stop_words and len(tok) > 1]
            if filtered:
                result.append(filtered)
        return result

    def lemmatize(self, tokens: List[str]) -> List[str]:
        return [self.lemmatizer.lemmatize(tok) for tok in tokens]

    def preprocess(self, text: str) -> List[List[str]]:
        cleaned = self.clean_gentle(text)
        sents = self.tokenize_sentences(cleaned)
        return [self.lemmatize(sent) for sent in sents]

    def preprocess_single_text(self, text: str) -> List[str]:
        sents = self.preprocess(text)
        return [tok for sent in sents for tok in sent]

    def flatten(self, text: str) -> str:
        return ' '.join(self.preprocess_single_text(text))

    def batch(self, texts: List[str]) -> List[List[List[str]]]:
        return [self.preprocess(txt) for txt in texts]

def demo_pipelines():

    # Sample film descriptions
    sample_texts = [
        "A young man with a troubled past becomes a superhero to save his city from corruption and crime. The movie follows his journey as he discovers his powers.",
        "In this romantic comedy, two strangers meet on a train and fall in love during their journey across Europe. It's a heartwarming story about finding love.",
        "This thriller tells the story of a detective who's investigating a series of mysterious murders. The plot thickens as he realizes he's being watched."
    ]

    # Initialize pipelines
    lsa_pipeline = LSASVDPipeline()
    wemb_pipeline = WordEmbeddingPipeline()

    print("=== LSA/SVD Pipeline Results ===")
    for i, text in enumerate(sample_texts):
        processed = lsa_pipeline.preprocess(text)
        print(f"Text {i+1}: {processed}")
        print()

    print("=== Word2Vec/GloVe/FastText Pipeline Results ===")
    for i, text in enumerate(sample_texts):
        processed = wemb_pipeline.preprocess_single_text(text)
        print(f"Text {i+1}: {processed}")
        processed = wemb_pipeline.preprocess(text)
        print(f"Text {i+1}: {processed}")


if __name__ == "__main__":
    demo_pipelines()

"""# Full pipeline

"""

from typing import Optional, List, Union
from pydantic import BaseModel, Field, HttpUrl, ValidationError, validator
from pymongo import UpdateOne

class FilmMetadata(BaseModel):
    film_name: str
    image_link: Optional[HttpUrl]
    is_adult: Optional[int]
    start_year: Optional[int]
    runtime_minutes: Optional[int]
    genres: Optional[str]
    rating: Optional[float]
    votes: Optional[int]
    directors: Optional[str]
    writers: Optional[str]

class RawFilm(BaseModel):
    _id: Optional[str] = None
    id: str
    film_name: Union[str, int, float]
    description: str
    image_link: Optional[HttpUrl] = None
    isAdult: Optional[int] = None
    startYear: Optional[int] = None
    runtimeMinutes: Optional[int] = None
    genres: Optional[str] = None
    rating: Optional[float] = None
    votes: Optional[int] = None
    directors: Optional[str] = None
    writers: Optional[str] = None

    @validator('film_name', pre=True)
    def ensure_str_film_name(cls, v):
        if v is None or str(v).strip() == "":
            raise ValueError("film_name is required and cannot be empty")
        return str(v)

class CleanFilm(BaseModel):
    id: str
    original_description: str
    cleaned_description: Union[str, List[str]]
    metadata: FilmMetadata

def process_and_upsert(films: List[dict], pipeline, dest_collection_name):
    # Connect to MongoDB client
    mongo_client = connect_to_mongodb(MONGO_URI)
    database = get_database(mongo_client, DATABASE_NAME)
    dest_collection = get_collection(database, dest_collection_name)

    ops = []
    validation_errors = 0

    # Getr all fields in raw data
    required_fields = list(RawFilm.model_fields.keys())

    for raw in films:
        # If raw data missed any fields -> assign None (avoid to loose data)
        for field in required_fields:
            if field not in raw:
                raw[field] = None

        try:
            rf = RawFilm(**raw)
        except ValidationError as e:
            validation_errors += 1
            print(f"Validation error on {raw.get('id')}: {e}")
            continue

        if hasattr(pipeline, "preprocess_single_text"):
            cleaned = pipeline.preprocess_single_text(rf.description)
        elif hasattr(pipeline, "preprocess"):
            cleaned = pipeline.preprocess(rf.description)
        else:
            raise Exception("Invalid pipeline!!!")

        if not cleaned:
            print(f"Warning: cleaned data empty or None for id {rf.id}")
            continue

        cf = CleanFilm(
            id=rf.id,
            original_description=rf.description,
            cleaned_description=cleaned,
            metadata=FilmMetadata(
                film_name=rf.film_name,
                image_link=rf.image_link,
                is_adult=rf.isAdult,
                start_year=rf.startYear,
                runtime_minutes=rf.runtimeMinutes,
                genres=rf.genres,
                rating=rf.rating,
                votes=rf.votes,
                directors=rf.directors,
                writers=rf.writers
            )
        )

        ops.append(
            UpdateOne(
                {"id": cf.id},
                {"$set": cf.model_dump(mode="json")},
                upsert=True
            )
        )

    print(f"Validation errors skipped: {validation_errors}")
    if ops:
        result = dest_collection.bulk_write(ops)
        print(f"Matched: {result.matched_count}, Inserted: {result.upserted_count}, Modified: {result.modified_count}")
    else:
        print("No operations to write.")

# Connect to MongoDB and get collection
mongo_client = connect_to_mongodb(MONGO_URI)
database = get_database(mongo_client, DATABASE_NAME)
collection = get_collection(database, COLLECTION_NAME)

# Define new collection names
lsa_collection_name = "lsa_svd_preprocessed"
wemb_collection_name = "word_embedding_preprocessed"

# Demo
all_documents = get_all_documents(collection)

# LSA/SVD pipeline
lsa = LSASVDPipeline()

process_and_upsert(
    films=all_documents,
    pipeline=lsa,
    dest_collection_name=lsa_collection_name
)

# WordEmbedding pipeline
wemb = WordEmbeddingPipeline()

process_and_upsert(
    films=all_documents,
    pipeline=wemb,
    dest_collection_name=wemb_collection_name
)

"""# Check preprocessed data

"""

# LSA/SVD collection
mongo_client = connect_to_mongodb(MONGO_URI)
database = get_database(mongo_client, DATABASE_NAME)
lsa_collection = get_collection(database, lsa_collection_name)

# Get some samples
lsa_samples = get_documents_by_index(lsa_collection, 0, 3)
for sample in lsa_samples:
    pprint(sample)

# LSA/SVD collection
mongo_client = connect_to_mongodb(MONGO_URI)
database = get_database(mongo_client, DATABASE_NAME)
wemb_collection = get_collection(database, wemb_collection_name)

# Get some samples
wemb_samples = get_documents_by_index(wemb_collection, 0, 3)
for sample in wemb_samples:
    pprint(sample)