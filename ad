[33mcommit c3570efffc884a5543f2636895b9795d8837662e[m[33m ([m[1;32mMinhHung--branch[m[33m)[m
Author: nmh1532 <hungminhnguyen15748@gmail.com>
Date:   Mon Jun 9 00:16:34 2025 +0700

    Create ppmi.py (embedding & train)

[1mdiff --git a/.gitattributes b/.gitattributes[m
[1mindex 9659883..1b0ad44 100644[m
[1m--- a/.gitattributes[m
[1m+++ b/.gitattributes[m
[36m@@ -1,2 +1,2 @@[m
 *.pkl filter=lfs diff=lfs merge=lfs -text[m
[31m-[m
[32m+[m[32mppmi.pkl filter=lfs diff=lfs merge=lfs -text[m
[1mdiff --git a/embedding/ppmi.py b/embedding/ppmi.py[m
[1mnew file mode 100644[m
[1mindex 0000000..b2a2888[m
[1m--- /dev/null[m
[1m+++ b/embedding/ppmi.py[m
[36m@@ -0,0 +1,215 @@[m
[32m+[m[32mimport os[m
[32m+[m[32mimport sys[m
[32m+[m[32mimport time[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mfrom collections import Counter[m
[32m+[m[32mfrom typing import List, Dict, Union, Optional[m
[32m+[m[32mfrom tqdm import tqdm[m
[32m+[m[32mfrom dotenv import load_dotenv[m
[32m+[m
[32m+[m[32msys.stdout.reconfigure(encoding='utf-8')[m
[32m+[m[32msys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))[m
[32m+[m[32mload_dotenv()[m
[32m+[m
[32m+[m[32mfrom database_connector.mongodb_connector import load_documents[m
[32m+[m
[32m+[m
[32m+[m[32mclass TruncatedSVD:[m
[32m+[m[32m    def __init__(self, n_components: Optional[int] = None) -> None:[m
[32m+[m[32m        self.n_components: Optional[int] = n_components[m
[32m+[m[32m        self.components_: Optional[np.ndarray] = None[m
[32m+[m[32m        self.singular_values_: Optional[np.ndarray] = None[m
[32m+[m[32m        self.explained_variance_ratio_: Optional[np.ndarray] = None[m
[32m+[m[32m        self.mean_: Optional[np.ndarray] = None[m
[32m+[m[32m        self.fitted: bool = False[m
[32m+[m
[32m+[m[32m    def fit(self, X: np.ndarray) -> None:[m
[32m+[m[32m        self.mean_ = np.mean(X, axis=0)[m
[32m+[m[32m        X_centered = X - self.mean_[m
[32m+[m[32m        U, S, VT = np.linalg.svd(X_centered, full_matrices=False)[m
[32m+[m
[32m+[m[32m        if self.n_components is None:[m
[32m+[m[32m            self.n_components = X.shape[1][m
[32m+[m
[32m+[m[32m        self.components_ = VT[:self.n_components][m
[32m+[m[32m        self.singular_values_ = S[:self.n_components][m
[32m+[m[32m        total_var = np.sum(S ** 2)[m
[32m+[m[32m        comp_var = S[:self.n_components] ** 2[m
[32m+[m[32m        self.explained_variance_ratio_ = comp_var / total_var[m
[32m+[m[32m        self.fitted = True[m
[32m+[m
[32m+[m[32m    def transform(self, X: np.ndarray) -> np.ndarray:[m
[32m+[m[32m        if not self.fitted:[m
[32m+[m[32m            raise RuntimeError("TruncatedSVD not fit.")[m
[32m+[m[32m        X_centered = X - self.mean_[m
[32m+[m[32m        return np.dot(X_centered, self.components_.T)[m
[32m+[m
[32m+[m[32m    def fit_transform(self, X: np.ndarray) -> np.ndarray:[m
[32m+[m[32m        self.fit(X)[m
[32m+[m[32m        return self.transform(X)[m
[32m+[m
[32m+[m[32m    def choose_n_components(self, threshold: float = 0.95) -> int:[m
[32m+[m[32m        if not self.fitted:[m
[32m+[m[32m            raise RuntimeError("Model does not fit data!")[m
[32m+[m[32m        cum_var = np.cumsum(self.explained_variance_ratio_)[m
[32m+[m[32m        n_comp = int(np.searchsorted(cum_var, threshold) + 1)[m
[32m+[m[32m        self.n_components = n_comp[m
[32m+[m[32m        self.components_ = self.components_[:n_comp][m
[32m+[m[32m        self.singular_values_ = self.singular_values_[:n_comp][m
[32m+[m[32m        self.explained_variance_ratio_ = self.explained_variance_ratio_[:n_comp][m
[32m+[m[32m        return n_comp[m
[32m+[m
[32m+[m[32m    def plot_cumulative_variance(self, threshold: float = 0.95) -> None:[m
[32m+[m[32m        if self.explained_variance_ratio_ is None:[m
[32m+[m[32m            raise RuntimeError("Call .fit() before plotting.")[m
[32m+[m[32m        cum_var = np.cumsum(self.explained_variance_ratio_)[m
[32m+[m[32m        plt.figure(figsize=(10, 6))[m
[32m+[m[32m        plt.plot(range(1, len(cum_var) + 1), cum_var, marker='o', linestyle='-')[m
[32m+[m[32m        if threshold is not None:[m
[32m+[m[32m            n_components = np.searchsorted(cum_var, threshold) + 1[m
[32m+[m[32m            plt.axvline(x=n_components, color='blue', linestyle='--', label=f'{n_components} components')[m
[32m+[m[32m            plt.axhline(y=threshold, color='red', linestyle='--', label=f'{int(threshold*100)}% variance')[m
[32m+[m[32m        plt.title("Cumulative Explained Variance by LSA Components (PPMI + LSA)")[m
[32m+[m[32m        plt.xlabel("Number of Components")[m
[32m+[m[32m        plt.ylabel("Cumulative Explained Variance Ratio")[m
[32m+[m[32m        plt.grid(True)[m
[32m+[m[32m        plt.legend()[m
[32m+[m[32m        plt.tight_layout()[m
[32m+[m[32m        plt.show()[m
[32m+[m
[32m+[m
[32m+[m[32mclass PPMIEmbedder:[m
[32m+[m[32m    def __init__(self, window_size: int = 2, max_features: Optional[int] = None, n_components: Optional[int] = None) -> None:[m
[32m+[m[32m        self.window_size: int = window_size[m
[32m+[m[32m        self.max_features: Optional[int] = max_features[m
[32m+[m[32m        self.n_components: Optional[int] = n_components[m
[32m+[m[32m        self.vocab: Dict[str, int] = {}[m
[32m+[m[32m        self.svd: TruncatedSVD = TruncatedSVD(n_components)[m
[32m+[m[32m        self.embeddings: Optional[np.ndarray] = None[m
[32m+[m[32m        self._original_ppmi: Optional[np.ndarray] = None[m
[32m+[m
[32m+[m[32m    def _tokenize(self, doc: Union[str, List[str]]) -> List[str]:[m
[32m+[m[32m        return doc.split() if isinstance(doc, str) else doc[m
[32m+[m
[32m+[m[32m    def _build_vocab(self, docs: List[Union[str, List[str]]]) -> None:[m
[32m+[m[32m        counter = Counter()[m
[32m+[m[32m        for doc in docs:[m
[32m+[m[32m            counter.update(self._tokenize(doc))[m
[32m+[m[32m        if self.max_features:[m
[32m+[m[32m            most_common = counter.most_common(self.max_features)[m
[32m+[m[32m            self.vocab = {word: i for i, (word, _) in enumerate(most_common)}[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.vocab = {word: i for i, word in enumerate(counter.keys())}[m
[32m+[m
[32m+[m[32m    def _build_cooc_matrix(self, docs: List[Union[str, List[str]]]) -> np.ndarray:[m
[32m+[m[32m        size = len(self.vocab)[m
[32m+[m[32m        matrix = np.zeros((size, size), dtype=np.float64)[m
[32m+[m[32m        for doc in tqdm(docs, desc="Build a Co-occurrence Matrix"):[m
[32m+[m[32m            tokens = self._tokenize(doc)[m
[32m+[m[32m            token_ids = [self.vocab[t] for t in tokens if t in self.vocab][m
[32m+[m[32m            for i, center in enumerate(token_ids):[m
[32m+[m[32m                start = max(i - self.window_size, 0)[m
[32m+[m[32m                end = min(i + self.window_size + 1, len(token_ids))[m
[32m+[m[32m                for j in range(start, end):[m
[32m+[m[32m                    if i == j:[m
[32m+[m[32m                        continue[m
[32m+[m[32m                    context = token_ids[j][m
[32m+[m[32m                    matrix[center, context] += 1.0[m
[32m+[m[32m        return matrix[m
[32m+[m
[32m+[m[32m    def _calculate_ppmi(self, M: np.ndarray, eps: float = 1e-8) -> np.ndarray:[m
[32m+[m[32m        total = np.sum(M)[m
[32m+[m[32m        row_sums = np.sum(M, axis=1, keepdims=True)[m
[32m+[m[32m        col_sums = np.sum(M, axis=0, keepdims=True)[m
[32m+[m[32m        p_wc = M / total[m
[32m+[m[32m        p_w = row_sums / total[m
[32m+[m[32m        p_c = col_sums / total[m
[32m+[m[32m        with np.errstate(divide='ignore'):[m
[32m+[m[32m            pmi = np.log((p_wc + eps) / (p_w @ p_c + eps))[m
[32m+[m[32m        return np.maximum(pmi, 0)[m
[32m+[m
[32m+[m[32m    def fit(self, docs: List[Union[str, List[str]]]) -> None:[m
[32m+[m[32m        self._build_vocab(docs)[m
[32m+[m[32m        cooc = self._build_cooc_matrix(docs)[m
[32m+[m[32m        ppmi = self._calculate_ppmi(cooc)[m
[32m+[m[32m        self.embeddings = self.svd.fit_transform(ppmi)[m
[32m+[m[32m        self._original_ppmi = ppmi[m
[32m+[m
[32m+[m[32m    def transform(self, docs: List[Union[str, List[str]]]) -> np.ndarray:[m
[32m+[m[32m        if self.embeddings is None:[m
[32m+[m[32m            raise RuntimeError("You need to call fit() first.")[m
[32m+[m[32m        cooc = self._build_cooc_matrix(docs)[m
[32m+[m[32m        ppmi = self._calculate_ppmi(cooc)[m
[32m+[m[32m        return self.svd.transform(ppmi)[m
[32m+[m
[32m+[m[32m    def transform_docs(self, docs: List[Union[str, List[str]]]) -> np.ndarray:[m
[32m+[m[32m        if self.embeddings is None or not self.vocab:[m
[32m+[m[32m            raise RuntimeError("You need to call fit() first.")[m
[32m+[m[32m        dim = self.embeddings.shape[1][m
[32m+[m[32m        doc_vectors: List[np.ndarray] = [][m
[32m+[m[32m        for doc in docs:[m
[32m+[m[32m            tokens = self._tokenize(doc)[m
[32m+[m[32m            vectors = [self.embeddings[self.vocab[t]] for t in tokens if t in self.vocab][m
[32m+[m[32m            doc_vec = np.mean(vectors, axis=0) if vectors else np.zeros(dim)[m
[32m+[m[32m            doc_vectors.append(doc_vec)[m
[32m+[m[32m        return np.array(doc_vectors)[m
[32m+[m
[32m+[m[32m    def choose_n_components(self, threshold: float = 0.95) -> int:[m
[32m+[m[32m        n_comp = self.svd.choose_n_components(threshold)[m
[32m+[m[32m        if self._original_ppmi is not None:[m
[32m+[m[32m            self.embeddings = self.svd.transform(self._original_ppmi)[m
[32m+[m[32m        return n_comp[m
[32m+[m
[32m+[m
[32m+[m[32mdef train_ppmi_lsa([m
[32m+[m[32m    docs: List[str],[m
[32m+[m[32m    max_features: Optional[int] = None,[m
[32m+[m[32m    save_path: Optional[str] = None,[m
[32m+[m[32m    find_suit_n_component: bool = True[m
[32m+[m[32m) -> PPMIEmbedder:[m
[32m+[m[32m    print(f"Starting PPMI training on {len(docs)} documents...")[m
[32m+[m[32m    if max_features is None:[m
[32m+[m[32m        max_features = int(np.sqrt(len(docs)))[m
[32m+[m[32m        print(f"Max_feature not specified, use sqrt(N): {max_features}")[m
[32m+[m[32m    else:[m
[32m+[m[32m        print(f"Using max_features = {max_features}")[m
[32m+[m[32m    embedder = PPMIEmbedder(window_size=4, n_components=None, max_features=max_features)[m
[32m+[m[32m    start_time = time.time()[m
[32m+[m[32m    embedder.fit(docs)[m
[32m+[m[32m    print(f"- Vocabulary size: {len(embedder.vocab)}")[m
[32m+[m[32m    print(f"- PPMI matrix shape:: {embedder._original_ppmi.shape}")[m
[32m+[m[32m    print(f"- Initial PPMI computation completed in {time.time() - start_time:.2f} seconds")[m
[32m+[m[32m    if find_suit_n_component:[m
[32m+[m[32m        print("Selecting suitable number of components based on 95% variance threshold...")[m
[32m+[m[32m        best_n = embedder.choose_n_components(threshold=0.95)[m
[32m+[m[32m        print(f"- Suitable number of components for 95% variance: {best_n}")[m
[32m+[m[32m        print("Re-training with optimal number of components...")[m
[32m+[m[32m        embedder = PPMIEmbedder(n_components=best_n, max_features=max_features)[m
[32m+[m[32m        start_time = time.time()[m
[32m+[m[32m        embedder.fit(docs)[m
[32m+[m[32m        print(f"Re-training completed in {time.time() - start_time:.2f} seconds")[m
[32m+[m[32m    save_path = "./embedding/trained_models/ppmi.pkl" if save_path is None else save_path[m
[32m+[m[32m    os.makedirs(os.path.dirname(save_path), exist_ok=True)[m
[32m+[m[32m    with open(save_path, "wb") as f:[m
[32m+[m[32m        pickle.dump(embedder, f)[m
[32m+[m[32m        print(f"Embedder saved to: {save_path}")[m
[32m+[m[32m    return embedder[m
[32m+[m
[32m+[m
[32m+[m[32mif __name__ == "__main__":[m
[32m+[m[32m    print("Connecting to MongoDB...")[m
[32m+[m[32m    uri: str = os.getenv("MONGO_URI")[m
[32m+[m[32m    db_name: str = os.getenv("DATABASE_NAME")[m
[32m+[m[32m    lsa_collection_name: str = os.getenv("LSA_COLLECTION_NAME")[m
[32m+[m[32m    lsa_docs = load_documents(uri, db_name, lsa_collection_name)[m
[32m+[m[32m    processed_lsa_docs: List[str] = [doc["cleaned_description"] for doc in lsa_docs][m
[32m+[m[32m    print(f"Loaded {len(processed_lsa_docs)} documents from MongoDB.")[m
[32m+[m[32m    embedder = train_ppmi_lsa([m
[32m+[m[32m        processed_lsa_docs,[m
[32m+[m[32m        max_features=1200,[m
[32m+[m[32m        save_path="./embedding/trained_models/ppmi.pkl",[m
[32m+[m[32m        find_suit_n_component=True[m
[32m+[m[32m    )[m
[32m+[m[32m    print("PPMI training pipeline completed.")[m
